{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dephinate/ASU/DL/MisterRetriveRite\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/home/dephinate/ASU/DL/MisterRetriveRite/\")\n",
    "! pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install python-box==6.0.2\n",
    "# ! pip install ensure==1.0.2\n",
    "# ! pip install -e .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from misterRetriveRite.config.configurations import ConfigurationManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-05 13:20:45,601,INFO,common,created directory at: artifacts]\n"
     ]
    }
   ],
   "source": [
    "config = ConfigurationManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelConfig(model_name='gpt-3.5-turbo-instruct', temperature=0.9, max_tokens=500, chunk_size=2000, chunk_overlap=100)\n",
      "[2024-03-05 13:20:46,235,INFO,common,created directory at: artifacts/vector_db]\n",
      "VectorizationConfig(root_dir='artifacts/vector_db', encoder_name='all-mpnet-base-v2', model_ckpt='None', data_path='None', k='None', num_of_cells='None', nprobe='None')\n"
     ]
    }
   ],
   "source": [
    "print(config.get_model_config())\n",
    "print(config.get_vectorization_config())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-05 13:20:50,547,INFO,utils,Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.]\n",
      "[2024-03-05 13:20:50,548,INFO,utils,NumExpr defaulting to 8 threads.]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "class DataLoader():\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    def load_from_url(self,urls: list):\n",
    "        loader = UnstructuredURLLoader(urls=urls)\n",
    "        data = loader.load()\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "class Splitter():\n",
    "    def __init__(self,data) -> None:\n",
    "        self.data = data\n",
    "        pass\n",
    "    def split_recursive(self, chunk_size:int,chunk_overlap:int,sperators:list[str]):\n",
    "        Splitter = RecursiveCharacterTextSplitter(\n",
    "            separators=sperators,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        splits = Splitter.split_documents(self.data)\n",
    "        return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from misterRetriveRite.utils.common import pickel_dump\n",
    "\n",
    "class Vectorizer():\n",
    "    def __init__(self,data) -> None:\n",
    "        self.data = data\n",
    "    def build_vectorindex_with_faiss_and_openai(self, save_to_local:bool,file_path:None):\n",
    "        from langchain.embeddings import OpenAIEmbeddings\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "        vectorindex = FAISS.from_documents(self.data, embeddings)\n",
    "        if save_to_local:\n",
    "            pickel_dump(file_path=file_path,data=vectorindex)\n",
    "            return vectorindex\n",
    "        return vectorindex\n",
    "    \n",
    "    def build_vectorindex_with_faiss_and_huggingface(self, model_name: str,save_to_local:bool,file_path:None):\n",
    "        from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
    "        embeddings = HuggingFaceBgeEmbeddings(model_name = model_name)\n",
    "        vectorindex = FAISS.from_documents(self.data, embeddings)\n",
    "        if save_to_local:\n",
    "            pickel_dump(file_path=file_path,data=vectorindex)\n",
    "            return vectorindex\n",
    "        return vectorindex\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "equityAnalyst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
